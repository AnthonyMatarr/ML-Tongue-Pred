{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4c5cd7",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2845e88",
   "metadata": {},
   "source": [
    "## Set globals + initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9618d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.data_utils import get_feature_lists\n",
    "from src.config import SEED, BASE_PATH\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "outcome_df = pd.read_excel(BASE_PATH / \"data\" / \"processed\" / \"Outcome_df.xlsx\")\n",
    "X_df = pd.read_excel(\n",
    "    BASE_PATH / \"data\" / \"processed\" / \"fully_cleaned_tongue_data.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bd478",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [\n",
    "    # Pre-op\n",
    "    \"SEX\",  # Nominal\n",
    "    \"RACE_NEW\",  # Nominal\n",
    "    \"ETHNICITY_HISPANIC\",  # Binary\n",
    "    \"INOUT\",  # Binary\n",
    "    \"Age\",  # Numerical\n",
    "    \"ELECTSURG\",  # Nominal\n",
    "    \"HEIGHT\",  # Numerical\n",
    "    \"WEIGHT\",  # Numerical\n",
    "    \"DIABETES\",  # Binary\n",
    "    \"SMOKE\",  # Binary\n",
    "    \"DYSPNEA\",  # Binary\n",
    "    \"FNSTATUS2\",\n",
    "    \"VENTILAT\",\n",
    "    \"HXCOPD\",\n",
    "    \"ASCITES\",\n",
    "    \"HXCHF\",\n",
    "    \"HYPERMED\",\n",
    "    \"RENAFAIL\",\n",
    "    \"DIALYSIS\",\n",
    "    \"DISCANCR\",\n",
    "    \"WNDINF\",\n",
    "    \"STEROID\",\n",
    "    \"WTLOSS\",\n",
    "    \"BLEEDDIS\",\n",
    "    \"TRANSFUS\",\n",
    "    \"PRSEPIS\",\n",
    "    \"PRALBUM\",\n",
    "    \"PRWBC\",\n",
    "    \"ASACLAS\",\n",
    "    # Intra-op\n",
    "    \"OPTIME\",\n",
    "    \"Partial Glossectomy (Hemiglossectomy_Subtotal)\",\n",
    "    \"Composite_Extended Glossectomy\",\n",
    "    \"Total Glossectomy (Complete Tongue Removal)\",\n",
    "    \"Excision of Tongue Lesions (Minor)\",\n",
    "    \"Local_Regional Tissue Flaps for Oral Cavity Reconstruction\",\n",
    "    \"Free Tissue Transfer (Microvascular Free Flaps) and Complex Flap Reconstruction\",\n",
    "    \"Skin Autografts for Head and Neck Reconstruction\",\n",
    "    \"Neck Dissection and Lymphadenectomy Procedures\",\n",
    "    \"Alveolar Ridge and Gingival Procedures\",\n",
    "    \"Mandibular Resection and Reconstruction Procedures\",\n",
    "    \"Peripheral Nerve Repair and Neuroplasty\",\n",
    "    \"Tracheostomy Procedures\",\n",
    "    \"Gastrostomy and Esophageal Access Procedures\",\n",
    "    \"Submandibular Gland Excision\",\n",
    "    \"Parotid Gland Excision\",\n",
    "    \"Laryngeal Resection and Reconstruction Procedures\",\n",
    "    \"Pharyngeal Resection and Reconstruction Procedures\",\n",
    "    \"Tonsillectomy and Tonsillar Region Procedures\",\n",
    "    \"Malignant neoplasm\",\n",
    "]\n",
    "\n",
    "X_sub = X_df[x_cols].copy()\n",
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a91e2",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef228bb8",
   "metadata": {},
   "source": [
    "Initialize feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imported func from src\n",
    "feature_lists = get_feature_lists(X_sub)\n",
    "binary_cols = feature_lists[\"binary_cols\"]\n",
    "numerical_cols = feature_lists[\"numerical_cols\"]\n",
    "nominal_cols = feature_lists[\"nominal_cols\"]\n",
    "ordinal_cols = feature_lists[\"ordinal_cols\"]\n",
    "X_sub[\"FNSTATUS2\"] = (\n",
    "    X_sub[\"FNSTATUS2\"].replace({\"Independent\": \"1\", \"Dependent\": \"0\"}).astype(int)\n",
    ")\n",
    "X_sub[\"SEX\"] = X_sub[\"SEX\"].replace({\"male\": \"1\", \"female\": \"0\"}).astype(int)\n",
    "X_sub[binary_cols] = (\n",
    "    X_sub[binary_cols]\n",
    "    .replace({\"Yes\": \"1\", \"No\": \"0\", \"No_Unknown\": \"0\"})\n",
    "    .apply(pd.to_numeric)\n",
    ")\n",
    "\n",
    "len(binary_cols) + len(nominal_cols) + len(ordinal_cols) + len(\n",
    "    numerical_cols\n",
    ") == X_sub.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a288006",
   "metadata": {},
   "source": [
    "Construct Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMICalculatorArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, height_idx, weight_idx):\n",
    "        self.height_idx = height_idx\n",
    "        self.weight_idx = weight_idx\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        height = X[:, self.height_idx]\n",
    "        weight = X[:, self.weight_idx]\n",
    "        bmi = (weight * 703) / (height**2)\n",
    "        # Remove height and weight columns\n",
    "        mask = np.ones(X.shape[1], dtype=bool)\n",
    "        mask[[self.height_idx, self.weight_idx]] = False\n",
    "        X_new = X[:, mask]\n",
    "        # Append BMI as last column\n",
    "        X_new = np.column_stack([X_new, bmi])\n",
    "        return X_new.astype(np.float32)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # Remove height and weight, add BMI\n",
    "        if input_features is None:\n",
    "            input_features = [\n",
    "                f\"num_{i}\" for i in range(self.height_idx + self.weight_idx + 1)\n",
    "            ]\n",
    "        input_features = list(input_features)\n",
    "        # Remove height and weight\n",
    "        features = [\n",
    "            f\n",
    "            for i, f in enumerate(input_features)\n",
    "            if i not in [self.height_idx, self.weight_idx]\n",
    "        ]\n",
    "        features.append(\"BMI\")\n",
    "        return np.array(features)\n",
    "\n",
    "\n",
    "def remove_prefix(df):\n",
    "    X = df.copy()\n",
    "    X.columns = X.columns.str.replace(r\"^\\w+__\", \"\", regex=True)\n",
    "    return X\n",
    "\n",
    "\n",
    "# wnd_class_order = ['1-Clean', '2-Clean/Contaminated', '3-Contaminated', '4-Dirty/Infected']\n",
    "asa_class_order = [\n",
    "    \"1-No Disturb\",\n",
    "    \"2-Mild Disturb\",\n",
    "    \"3-Severe Disturb\",\n",
    "    \"4-Life Threat\",\n",
    "]\n",
    "\n",
    "height_idx = numerical_cols.index(\"HEIGHT\")\n",
    "weight_idx = numerical_cols.index(\"WEIGHT\")\n",
    "\n",
    "# Numerical pipeline: Impute, calculate BMI, then scale\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", IterativeImputer(random_state=SEED, sample_posterior=True)),\n",
    "        (\"bmi\", BMICalculatorArray(height_idx=height_idx, weight_idx=weight_idx)),\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Categorical pipeline: OneHotEncode\n",
    "nom_pipeline = Pipeline([(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "# Ordinal Pipeline for asa class\n",
    "asa_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"encoder\",\n",
    "            OrdinalEncoder(\n",
    "                categories=[asa_class_order],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=-1,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine all preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipeline, numerical_cols),\n",
    "        (\"cat\", nom_pipeline, nominal_cols),\n",
    "        # ('ord_wnd', wnd_pipeline, [ordinal_cols[0]]),\n",
    "        (\"ord_asa\", asa_pipeline, [ordinal_cols[0]]),\n",
    "        (\"bin\", \"passthrough\", binary_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7da1b",
   "metadata": {},
   "source": [
    "Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_export_data(X, y, sample_ratio, file_path=None):\n",
    "    ##Get train set\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=SEED, stratify=y\n",
    "    )\n",
    "    ##Get val + test set\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    preprocessor.fit(X_train)\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "    X_train_transformed = np.array(preprocessor.transform(X_train))\n",
    "    X_train_transformed = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "    X_train_transformed = remove_prefix(X_train_transformed)\n",
    "\n",
    "    X_val_transformed = np.array(preprocessor.transform(X_val))\n",
    "    X_val_transformed = pd.DataFrame(X_val_transformed, columns=feature_names)\n",
    "    X_val_transformed = remove_prefix(X_val_transformed)\n",
    "\n",
    "    X_test_transformed = np.array(preprocessor.transform(X_test))\n",
    "    X_test_transformed = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "    X_test_transformed = remove_prefix(X_test_transformed)\n",
    "\n",
    "    # Reset index\n",
    "    X_train_transformed.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    X_val_transformed.reset_index(drop=True, inplace=True)\n",
    "    y_val.reset_index(drop=True, inplace=True)\n",
    "    X_test_transformed.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for col in X_train_transformed.columns:\n",
    "        try:\n",
    "            X_train_transformed[col] = pd.to_numeric(X_train_transformed[col])\n",
    "        except Exception as e:\n",
    "            print(f\"Column {col} failed: {e}\")\n",
    "\n",
    "    for col in X_val_transformed.columns:\n",
    "        try:\n",
    "            X_val_transformed[col] = pd.to_numeric(X_val_transformed[col])\n",
    "        except Exception as e:\n",
    "            print(f\"Column {col} failed: {e}\")\n",
    "\n",
    "    for col in X_test_transformed.columns:\n",
    "        try:\n",
    "            X_val_transformed[col] = pd.to_numeric(X_val_transformed[col])\n",
    "        except Exception as e:\n",
    "            print(f\"Column {col} failed: {e}\")\n",
    "\n",
    "    ######### CREATE OVER SAMPLED SET ##########\n",
    "    og_nom_cols = get_feature_lists(X_train_transformed)[\"binary_cols\"]\n",
    "    categorical_indices = [\n",
    "        i for i, c in enumerate(X_train_transformed.columns) if c in og_nom_cols\n",
    "    ]\n",
    "    smote_nc = SMOTENC(\n",
    "        categorical_features=categorical_indices,\n",
    "        sampling_strategy=sample_ratio,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    train_res = smote_nc.fit_resample(X_train_transformed.values, y_train.values)\n",
    "    X_train_resampled, y_train_resampled = train_res[:2]\n",
    "    X_train_resampled = pd.DataFrame(\n",
    "        X_train_resampled, columns=X_train_transformed.columns\n",
    "    )\n",
    "    y_train_resampled = pd.Series(np.array(y_train_resampled))\n",
    "    ######### CREATE OVER SAMPLED SET ##########\n",
    "\n",
    "    if file_path:\n",
    "        if file_path.exists():\n",
    "            rmtree(file_path)\n",
    "        file_path.mkdir(exist_ok=False, parents=True)\n",
    "        X_train_transformed.to_parquet(file_path / \"X_train.parquet\")\n",
    "        y_train.to_excel(file_path / \"y_train.xlsx\")\n",
    "        X_train_resampled.to_parquet(file_path / \"X_train_res.parquet\")\n",
    "        y_train_resampled.to_excel(file_path / \"y_train_res.xlsx\")\n",
    "        X_val_transformed.to_parquet(file_path / \"X_val.parquet\")\n",
    "        y_val.to_excel(file_path / \"y_val.xlsx\")\n",
    "        X_test_transformed.to_parquet(file_path / \"X_test.parquet\")\n",
    "        y_test.to_excel(file_path / \"y_test.xlsx\")\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train_transformed,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_train_res\": X_train_resampled,\n",
    "        \"y_train_res\": y_train_resampled,\n",
    "        \"X_val\": X_val_transformed,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test_transformed,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "\n",
    "\n",
    "data_path = BASE_PATH / \"data\" / \"processed\"\n",
    "# 8.4% w/o over sampling\n",
    "# AKA 0.09 pos/neg\n",
    "surg_data = transform_export_data(\n",
    "    X_sub, outcome_df[\"Surgical_Outcome\"], 0.27, data_path / \"outcome_surg\"\n",
    ")\n",
    "# 10% w/o over sampling\n",
    "# AKA 0.11 pos/neg\n",
    "bleed_data = transform_export_data(\n",
    "    X_sub, outcome_df[\"Bleed_Outcome\"], 0.33, data_path / \"outcome_bleed\"\n",
    ")\n",
    "# 7.05% w/o over sampling\n",
    "# AKA 0.07 pos/neg\n",
    "asp_data = transform_export_data(\n",
    "    X_sub, outcome_df[\"Aspiration_Outcome\"], 0.21, data_path / \"outcome_asp\"\n",
    ")\n",
    "# 0.9% w/o over sampling\n",
    "# AKA 0.01 pos/neg\n",
    "X_sub_mort = X_sub.drop(\"WNDINF\", axis=1)\n",
    "mort_data = transform_export_data(\n",
    "    X_sub, outcome_df[\"Mortality_Outcome\"], 0.1, data_path / \"outcome_mort\"\n",
    ")\n",
    "\n",
    "# outcome_data = {\n",
    "#     'Surgical Wound Complication': surg_data,\n",
    "#     'Bleed': bleed_data,\n",
    "#     'Aspiration Complications': asp_data,\n",
    "#     'Mortality': mort_data\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-tongue-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
